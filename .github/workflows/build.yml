name: Build, Sonar + AI Analysis

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  analyze:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      issues: write

    env:
      SONAR_URL: https://sonarcloud.io
      PROJECT_KEY: Raygama_collections-demo

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          ref: ${{ github.event.pull_request.head.ref }}

      - name: Set up Maven & JDK17
        uses: actions/setup-java@v3
        with:
          java-version: 17
          distribution: temurin

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-maven-

      - name: Compile
        run: mvn -B compile -Drat.skip=true -DskipTests

      - name: SonarCloud scan
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mvn -B org.sonarsource.scanner.maven:sonar-maven-plugin:sonar \
            -Dsonar.projectKey=${{ env.PROJECT_KEY }} \
            -Dsonar.organization=raygama \
            -Dsonar.host.url=${{ env.SONAR_URL }}

      - name: Install CLI tools
        run: sudo apt-get update && sudo apt-get install -y jq gh

      - name: Get PR changed files
        id: files
        env:
          GH_TOKEN: ${{ github.token }}
        shell: bash
        run: |
          set -euo pipefail

          # 1) List changed files for *this* PR explicitly
          gh pr diff ${{ github.event.pull_request.number }} --name-only > changed.txt || true
          # Normalize to JSON array (possibly empty)
          jq -R -s -c 'split("\n") | map(select(length>0))' changed.txt > changed.json

          # 2) Build list of files mentioned by issues (may not exist yet)
          if [ -f issues.json ]; then
            jq -c 'map(.component) | unique' issues.json > issue_files.json
          else
            echo '[]' > issue_files.json
          fi

          # 3) Intersection: changed files that appear in issues.json (jq 1.6-safe)
          INTERSECT=$(
            jq -c --slurpfile changed changed.json --slurpfile iss issue_files.json '
              ($iss[0] // []) as $issf |
              ($changed[0] // []) as $chg |
              [ $chg[] | select( ($issf | index(.)) != null ) ]
            ' /dev/null
          )
          # Fallback if something went weird
          if [ -z "${INTERSECT:-}" ] || ! jq -e . >/dev/null 2>&1 <<<"$INTERSECT"; then
            INTERSECT='[]'
          fi

          # 4) Up to 20 extra changed files not already in INTERSECT
          EXTRA=$(
            jq -c --slurpfile changed changed.json --argjson base "$INTERSECT" '
              ($changed[0] // []) as $all |
              [ $all[]
                | select( ($base | index(.)) == null )
              ][0:20]
            ' /dev/null
          )
          if [ -z "${EXTRA:-}" ] || ! jq -e . >/dev/null 2>&1 <<<"$EXTRA"; then
            EXTRA='[]'
          fi

          # 5) Final list = intersect + extra
          FINAL=$(jq -c --argjson a "$INTERSECT" --argjson b "$EXTRA" '$a + $b' <<< '{}')
          if [ -z "${FINAL:-}" ] || ! jq -e . >/dev/null 2>&1 <<<"$FINAL"; then
            FINAL='[]'
          fi

          echo "$FINAL" > changed.json

          echo "======= DEBUG: CHANGED FILES (focused) ======="
          echo "INTERSECT count: $(jq 'length' <<<"$INTERSECT")"
          echo "EXTRA count:     $(jq 'length' <<<"$EXTRA")"
          echo "FINAL count:     $(jq 'length' changed.json)"

          # expose for later steps
          echo "list=$(cat changed.json)" >> "$GITHUB_OUTPUT"
          jq length changed.json




      - name: Fetch Sonar issues + code snippets (token-safe)
        id: sonar-issues
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          set -euo pipefail

          # ---- Tunables (tighten if you still hit limits) ----
          MAX_ISSUES=120            # hard cap on issues you will pass to AI
          MAX_ISSUES_PER_RULE=25    # avoid one rule dominating
          MAX_ISSUES_PER_FILE=20    # avoid one file dominating
          MAX_SNIPPET_CHARS=380     # keep snippet tiny but useful
          CONTEXT_BEFORE=1          # lines before the hit
          CONTEXT_AFTER=1           # lines after the hit
          SONAR_SEVERITIES="BLOCKER,CRITICAL,MAJOR"   # you can widen later
          SONAR_TYPES="BUG,VULNERABILITY,CODE_SMELL"  # or narrow if needed
          # ----------------------------------------------------

          # Paginate from Sonar; stop when we collected >= MAX_ISSUES raw
          echo "[]" > all-issues.json
          PAGE=1
          PAGESIZE=500
          COLLECTED=0

          while : ; do
            PAGE_JSON=$(curl -s -G "${{ env.SONAR_URL }}/api/issues/search" \
              --data-urlencode "componentKeys=${{ env.PROJECT_KEY }}" \
              --data-urlencode "pullRequest=${{ github.event.pull_request.number }}" \
              --data-urlencode "statuses=OPEN" \
              --data-urlencode "severities=${SONAR_SEVERITIES}" \
              --data-urlencode "types=${SONAR_TYPES}" \
              --data-urlencode "p=${PAGE}" \
              --data-urlencode "ps=${PAGESIZE}" \
              -H "Authorization: Bearer ${{ secrets.SONAR_TOKEN }}")

            COUNT=$(echo "$PAGE_JSON" | jq '.issues | length')
            echo "Fetched page $PAGE with $COUNT issues"
            if [ "$COUNT" -eq 0 ]; then
              break
            fi

            # append into all-issues.json
            TMP=$(mktemp)
            jq -c --slurp 'add' \
              <(cat all-issues.json) \
              <(echo "$PAGE_JSON" | jq '.issues') > "$TMP"
            mv "$TMP" all-issues.json

            COLLECTED=$(jq 'length' all-issues.json)
            if [ "$COLLECTED" -ge "$MAX_ISSUES" ]; then
              echo "Collected >= MAX_ISSUES ($MAX_ISSUES). Stopping pagination."
              break
            fi
            PAGE=$((PAGE+1))
          done

          # If nothing, bail early
          if [ "$(jq 'length' all-issues.json)" -eq 0 ]; then
            echo '[]' > issues.json
            echo "has_issues=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Normalize / dedupe / prioritize:
          # - Keep only needed fields
          # - Order by severity strength, then by rule occurrence
          # - Cap per-rule and per-file
          PRIORITIZED=$(
            jq -c '
              # convert severity to a rank
              def sev_rank:
                if .severity=="BLOCKER" then 5
                elif .severity=="CRITICAL" then 4
                elif .severity=="MAJOR" then 3
                elif .severity=="MINOR" then 2
                else 1 end;

              map({
                key: .key,
                rule: .rule,
                severity: .severity,
                sevRank: (sev_rank),
                component: (.component | split(":")[1:] | join(":")),
                line: (.line // 1),
                message: .message
              })
              # dedupe by (rule, component, line, message)
              | unique_by([.rule, .component, .line, .message])
              # primary sort: severity rank desc
              | sort_by(-.sevRank, .rule, .component, .line)
            ' all-issues.json
          )

          # Enforce per-rule & per-file caps (use flatten so we never leave nested arrays)
          TRIMMED=$(
            echo "$PRIORITIZED" \
            | jq -c --argjson maxPerRule "$MAX_ISSUES_PER_RULE" --argjson maxPerFile "$MAX_ISSUES_PER_FILE" '
                (group_by(.rule)       | map(.[0:$maxPerRule])       | flatten) as $byRule
                | ($byRule | group_by(.component) | map(.[0:$maxPerFile]) | flatten)
              '
          )

          # Apply global cap
          TRIMMED=$(echo "$TRIMMED" | jq -c --argjson maxIssues "$MAX_ISSUES" '.[0:$maxIssues]')


          echo "$TRIMMED" > trimmed-issues.json
          echo "After trimming: $(jq 'length' trimmed-issues.json) issues"

          # Build compact snippets with tiny context and char cap
          echo '[' > issues.json
          FIRST=true
          while IFS= read -r issue; do
            comp=$(jq -r '.component' <<<"$issue")
            ln=$(jq -r '.line' <<<"$issue")

            # Skip if file not in workspace
            if [ ! -f "$comp" ]; then
              continue
            fi

            start=$((ln - ${CONTEXT_BEFORE})); [ $start -lt 1 ] && start=1
            end=$((ln + ${CONTEXT_AFTER}))
            snippet=$(sed -n "${start},${end}p" "$comp" \
                      | sed 's/"/\\"/g' \
                      | sed ':a;N;$!ba;s/\n/\\n/g' \
                      | cut -c1-"$MAX_SNIPPET_CHARS")

            entry=$(jq -n --argjson i "$issue" --arg snip "$snippet" '$i + {snippet: $snip}')
            if $FIRST; then FIRST=false; echo "$entry" >> issues.json; else echo ",$entry" >> issues.json; fi
          done < <(jq -c '.[]' trimmed-issues.json)
          echo ']' >> issues.json

          HAS_ISSUES=$(jq 'length > 0' issues.json)
          echo "has_issues=$HAS_ISSUES" >> $GITHUB_OUTPUT

      - name: debug inspect issues.json shapes
        shell: bash
        run: |
          set -euo pipefail

          echo "===== RAW issues.json (first 4000 bytes) ====="
          head -c 4000 issues.json || true
          echo
          echo "===== Top-level type & length ====="
          jq -r 'type as $t | "type=\($t) len=\(if $t=="array" then length else 1 end)"' issues.json

          echo "===== If object, show keys ====="
          jq -r 'if type=="object" then ("keys: " + (keys|join(", "))) else "not an object" end' issues.json

          echo "===== If array, show element types histogram ====="
          jq -r 'if type=="array" then (map(type) | reduce .[] as $t ({}; .[$t] += 1) | to_entries | map("\(.key): \(.value)") | .[] ) else "not an array" end' issues.json

          echo "===== First 5 non-objects (if top-level is array) ====="
          jq -c 'if type=="array" then [ .[] | select(type!="object") ][0:5] else [] end' issues.json

          echo "===== First 5 objects missing rule or component ====="
          jq -c 'if type=="array" then [ .[] | select(type=="object" and ( (has("rule")|not) or (has("component")|not) )) ][0:5] else [] end' issues.json

          echo "===== First 5 objects where rule/component are not strings ====="
          jq -c 'if type=="array" then [ .[] | select(type=="object" and ((.rule|type)!="string" or (.component|type)!="string")) ][0:5] else [] end' issues.json


      - name: Build compact issues + tiny summary (from trimmed-issues.json)
        shell: bash
        run: |
          set -euo pipefail

          # Tunables
          MAX_TOTAL_ISSUES=60
          MAX_PER_RULE=10
          MAX_PER_FILE=5
          NON_TEST_ONLY=true
          RULE_BLOCKLIST='["java:S1604","java:S108"]'
          MAX_EXEMPLARS_PER_RULE=1

          SRC=trimmed-issues.json
          if [ ! -s "$SRC" ]; then
            echo "[]" > issues_compact.json
            echo "No issues to summarize." > question_summary.txt
            exit 0
          fi

          echo "Source length: $(jq 'length' "$SRC")"

          # Normalize every element to a small object (drop non-objects, drop nested arrays)
          NORM=$(
            jq -c '
              ( if type=="array" then . else [.] end )
              | map(
                  if    type=="object" then .
                  elif  type=="array"  then empty         # drop any accidental nested arrays
                  else  empty
                  end
                )
              | map({
                  rule:      (.rule      // ""),
                  component: (.component // ""),
                  severity:  (.severity  // "UNKNOWN"),
                  line:      (.line      // 0),
                  message:   (.message   // "")
                })
            ' "$SRC"
          )
          echo "Normalized length: $(jq 'length' <<<"$NORM")"

          # Filter (non-empty rule/component, optional drop tests, blocklist)
          FILTERED=$(
            jq -c \
              --argjson block "$RULE_BLOCKLIST" \
              --arg ntest "$NON_TEST_ONLY" '
                [ .[]
                  | select(.rule != "" and .component != "")
                  | select( ( ($ntest=="true") | not ) or ( (.component | test("^src/test/") | not) ) )
                  | select( ($block | index(.rule)) == null )
                ]
              ' <<<"$NORM"
          )
          echo "Filtered length: $(jq 'length' <<<"$FILTERED")"

          if [ "$(jq 'length' <<<"$FILTERED")" -eq 0 ]; then
            echo "[]" > issues_compact.json
            echo "No eligible issues after filtering." > question_summary.txt
            exit 0
          fi

          # Sort & cap
          SORTED=$(
            jq -c '
              def rank:
                if .severity=="BLOCKER" then 3
                elif .severity=="CRITICAL" then 2
                elif .severity=="MAJOR" then 1
                else 0 end;
              sort_by([ - (rank), .rule, .component, (.line // 0) ])
            ' <<<"$FILTERED"
          )

          CAPPED=$(
            jq -c \
              --argjson maxRule "$MAX_PER_RULE" \
              --argjson maxFile "$MAX_PER_FILE" '
                (group_by(.rule) | map(.[0:$maxRule]) | flatten) as $byRule
                | ($byRule | group_by(.component) | map(.[0:$maxFile]) | flatten)
              ' <<<"$SORTED" \
            | jq -c --argjson m "$MAX_TOTAL_ISSUES" '.[0:$m]'
          )

          echo "$CAPPED" > issues_compact.json
          echo "Compact issue count: $(jq 'length' issues_compact.json)"

          # Tiny per-rule summary
          jq -r --argjson k "$MAX_EXEMPLARS_PER_RULE" '
            group_by(.rule)
            | map({
                rule: .[0].rule,
                severity: (.[0].severity // "UNKNOWN"),
                count: length,
                samples: (.[0:$k] | map("\(.component):\(.line)"))
              })
            | sort_by(.rule)
            | map("• \(.rule) [\(.severity)] — \(.count) hits; e.g. \(.samples|join(", "))")
            | .[]
          ' issues_compact.json > question_summary.txt

          echo "Summary preview:"
          head -n 10 question_summary.txt || true


      - name: Send issues to Technical Debt Analyzer (compact + robust)
        env:
          FLOWISE_BEARER_TOKEN: ${{ secrets.FLOWISE_BEARER_TOKEN }}
        run: |
          set -euo pipefail

          # Tiny question text built from the summary file
          BASE_HDR="You're analyzing SonarCloud issues from PR #${{ github.event.pull_request.number }} in '${{ github.repository }}' (owner: '${{ github.repository_owner }}'), targeting '${{ github.event.pull_request.head.ref }}'. Analyze duplicates once."
          SUMMARY=$(tr '\n' '\\n' < question_summary.txt | head -c 14000)
          QUESTION_TEXT="$BASE_HDR\\n\\n## Issue summary (by rule)\\n$SUMMARY"

          # Use compact JSON
          ISSUES_JSON=$(cat issues_compact.json)
          CHANGED_FILES=$(cat changed.json)

          make_payload () {
            jq -cn \
              --arg pr "${{ github.event.pull_request.number }}" \
              --arg proj "${{ env.PROJECT_KEY }}" \
              --arg question "$QUESTION_TEXT" \
              --argjson issues "$1" \
              --argjson changedFiles "$2" \
              '{question:$question, overrideConfig:{projectKey:$proj, prNumber:$pr, changedFiles:$changedFiles}, issues:$issues}'
          }

          PAYLOAD=$(make_payload "$ISSUES_JSON" "$CHANGED_FILES")

          # Send with robust handling
          RESP_BODY=flowise_output_raw.txt
          HTTP_CODE=$(curl -sS -o "$RESP_BODY" -w "%{http_code}" \
            -H "Authorization: Bearer $FLOWISE_BEARER_TOKEN" \
            -H "Content-Type: application/json" \
            -H "Accept: application/json" \
            --max-time 55 \
            --retry 2 --retry-delay 2 --retry-max-time 60 \
            -d "$PAYLOAD" \
            https://cloud.flowiseai.com/api/v1/prediction/ae8b5cb1-b90b-4f25-9395-a839e92e2bf6)

          echo "HTTP status: $HTTP_CODE"
          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ] && jq -e . "$RESP_BODY" >/dev/null 2>&1; then
            cp "$RESP_BODY" flowise_output.json
          else
            RAW=$(head -c 6000 "$RESP_BODY" | sed 's/"/\\"/g' | sed ':a;N;$!ba;s/\n/\\n/g')
            printf '{"text":"Flowise response issue (HTTP %s). Body (first 6KB):\\n```\\n%s\\n```"}' "$HTTP_CODE" "$RAW" > flowise_output.json
          fi

      - name: Comment on PR
        uses: actions/github-script@v6
        if: success()
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = 'flowise_output.json';

            if (!fs.existsSync(path)) {
              core.warning('Flowise output file not found.');
              return;
            }

            const raw = fs.readFileSync(path, 'utf8').trim();
            if (!raw || raw[0] !== '{') {
              core.warning('Flowise did not return valid JSON. Skipping comment.');
              return;
            }

            let parsed;
            try {
              parsed = JSON.parse(raw);
            } catch (e) {
              core.warning(`JSON parse error: ${e.message}`);
              return;
            }

            const body = parsed.text || 'Flowise returned:\n```json\n' +
                          JSON.stringify(parsed, null, 2) + '\n```';

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number, 
              body
            });
