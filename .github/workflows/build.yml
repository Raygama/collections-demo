name: Build, Sonar + AI Analysis

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  analyze:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      issues: write

    env:
      SONAR_URL: https://sonarcloud.io
      PROJECT_KEY: Raygama_collections-demo

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          ref: ${{ github.event.pull_request.head.ref }}

      - name: Set up Maven & JDK17
        uses: actions/setup-java@v3
        with:
          java-version: 17
          distribution: temurin

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-maven-

      - name: Compile
        run: mvn -B compile -Drat.skip=true -DskipTests

      - name: SonarCloud scan
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mvn -B org.sonarsource.scanner.maven:sonar-maven-plugin:sonar \
            -Dsonar.projectKey=${{ env.PROJECT_KEY }} \
            -Dsonar.organization=raygama \
            -Dsonar.host.url=${{ env.SONAR_URL }}

      - name: Install CLI tools
        run: sudo apt-get update && sudo apt-get install -y jq gh

      - name: Get PR changed files
        id: files
        env:
          GH_TOKEN: ${{ github.token }}
        shell: bash
        run: |
          set -euo pipefail

          # 1) List changed files for *this* PR explicitly
          gh pr diff ${{ github.event.pull_request.number }} --name-only > changed.txt || true
          # Normalize to JSON array (possibly empty)
          jq -R -s -c 'split("\n") | map(select(length>0))' changed.txt > changed.json

          # 2) Build list of files mentioned by issues (may not exist yet)
          if [ -f issues.json ]; then
            jq -c 'map(.component) | unique' issues.json > issue_files.json
          else
            echo '[]' > issue_files.json
          fi

          # 3) Intersection: changed files that appear in issues.json (jq 1.6-safe)
          INTERSECT=$(
            jq -c --slurpfile changed changed.json --slurpfile iss issue_files.json '
              ($iss[0] // []) as $issf |
              ($changed[0] // []) as $chg |
              [ $chg[] | select( ($issf | index(.)) != null ) ]
            ' /dev/null
          )
          # Fallback if something went weird
          if [ -z "${INTERSECT:-}" ] || ! jq -e . >/dev/null 2>&1 <<<"$INTERSECT"; then
            INTERSECT='[]'
          fi

          # 4) Up to 20 extra changed files not already in INTERSECT
          EXTRA=$(
            jq -c --slurpfile changed changed.json --argjson base "$INTERSECT" '
              ($changed[0] // []) as $all |
              [ $all[]
                | select( ($base | index(.)) == null )
              ][0:20]
            ' /dev/null
          )
          if [ -z "${EXTRA:-}" ] || ! jq -e . >/dev/null 2>&1 <<<"$EXTRA"; then
            EXTRA='[]'
          fi

          # 5) Final list = intersect + extra
          FINAL=$(jq -c --argjson a "$INTERSECT" --argjson b "$EXTRA" '$a + $b' <<< '{}')
          if [ -z "${FINAL:-}" ] || ! jq -e . >/dev/null 2>&1 <<<"$FINAL"; then
            FINAL='[]'
          fi

          echo "$FINAL" > changed.json

          echo "======= DEBUG: CHANGED FILES (focused) ======="
          echo "INTERSECT count: $(jq 'length' <<<"$INTERSECT")"
          echo "EXTRA count:     $(jq 'length' <<<"$EXTRA")"
          echo "FINAL count:     $(jq 'length' changed.json)"

          # expose for later steps
          echo "list=$(cat changed.json)" >> "$GITHUB_OUTPUT"
          jq length changed.json




      - name: Fetch Sonar issues + code snippets (token-safe)
        id: sonar-issues
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          set -euo pipefail

          # ---- Tunables (tighten if you still hit limits) ----
          MAX_ISSUES=120            # hard cap on issues you will pass to AI
          MAX_ISSUES_PER_RULE=25    # avoid one rule dominating
          MAX_ISSUES_PER_FILE=20    # avoid one file dominating
          MAX_SNIPPET_CHARS=380     # keep snippet tiny but useful
          CONTEXT_BEFORE=1          # lines before the hit
          CONTEXT_AFTER=1           # lines after the hit
          SONAR_SEVERITIES="BLOCKER,CRITICAL,MAJOR"   # you can widen later
          SONAR_TYPES="BUG,VULNERABILITY,CODE_SMELL"  # or narrow if needed
          # ----------------------------------------------------

          # Paginate from Sonar; stop when we collected >= MAX_ISSUES raw
          echo "[]" > all-issues.json
          PAGE=1
          PAGESIZE=500
          COLLECTED=0

          while : ; do
            PAGE_JSON=$(curl -s -G "${{ env.SONAR_URL }}/api/issues/search" \
              --data-urlencode "componentKeys=${{ env.PROJECT_KEY }}" \
              --data-urlencode "pullRequest=${{ github.event.pull_request.number }}" \
              --data-urlencode "statuses=OPEN" \
              --data-urlencode "severities=${SONAR_SEVERITIES}" \
              --data-urlencode "types=${SONAR_TYPES}" \
              --data-urlencode "p=${PAGE}" \
              --data-urlencode "ps=${PAGESIZE}" \
              -H "Authorization: Bearer ${{ secrets.SONAR_TOKEN }}")

            COUNT=$(echo "$PAGE_JSON" | jq '.issues | length')
            echo "Fetched page $PAGE with $COUNT issues"
            if [ "$COUNT" -eq 0 ]; then
              break
            fi

            # append into all-issues.json
            TMP=$(mktemp)
            jq -c --slurp 'add' \
              <(cat all-issues.json) \
              <(echo "$PAGE_JSON" | jq '.issues') > "$TMP"
            mv "$TMP" all-issues.json

            COLLECTED=$(jq 'length' all-issues.json)
            if [ "$COLLECTED" -ge "$MAX_ISSUES" ]; then
              echo "Collected >= MAX_ISSUES ($MAX_ISSUES). Stopping pagination."
              break
            fi
            PAGE=$((PAGE+1))
          done

          # If nothing, bail early
          if [ "$(jq 'length' all-issues.json)" -eq 0 ]; then
            echo '[]' > issues.json
            echo "has_issues=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Normalize / dedupe / prioritize:
          # - Keep only needed fields
          # - Order by severity strength, then by rule occurrence
          # - Cap per-rule and per-file
          PRIORITIZED=$(
            jq -c '
              # convert severity to a rank
              def sev_rank:
                if .severity=="BLOCKER" then 5
                elif .severity=="CRITICAL" then 4
                elif .severity=="MAJOR" then 3
                elif .severity=="MINOR" then 2
                else 1 end;

              map({
                key: .key,
                rule: .rule,
                severity: .severity,
                sevRank: (sev_rank),
                component: (.component | split(":")[1:] | join(":")),
                line: (.line // 1),
                message: .message
              })
              # dedupe by (rule, component, line, message)
              | unique_by([.rule, .component, .line, .message])
              # primary sort: severity rank desc
              | sort_by(-.sevRank, .rule, .component, .line)
            ' all-issues.json
          )

          # Enforce per-rule & per-file caps
          TRIMMED=$(
            echo "$PRIORITIZED" | jq -c --argjson maxPerRule "$MAX_ISSUES_PER_RULE" --argjson maxPerFile "$MAX_ISSUES_PER_FILE" '
              # group, then take first N from each bucket to avoid domination
              (group_by(.rule) | map(.[0:$maxPerRule]) | add) as $byRule
              | ($byRule | group_by(.component) | map(.[0:$maxPerFile]) | add)
            '
          )

          # Apply global cap
          TRIMMED=$(echo "$TRIMMED" | jq -c --argjson maxIssues "$MAX_ISSUES" '.[0:$maxIssues]')

          echo "$TRIMMED" > trimmed-issues.json
          echo "After trimming: $(jq 'length' trimmed-issues.json) issues"

          # Build compact snippets with tiny context and char cap
          echo '[' > issues.json
          FIRST=true
          while IFS= read -r issue; do
            comp=$(jq -r '.component' <<<"$issue")
            ln=$(jq -r '.line' <<<"$issue")

            # Skip if file not in workspace
            if [ ! -f "$comp" ]; then
              continue
            fi

            start=$((ln - ${CONTEXT_BEFORE})); [ $start -lt 1 ] && start=1
            end=$((ln + ${CONTEXT_AFTER}))
            snippet=$(sed -n "${start},${end}p" "$comp" \
                      | sed 's/"/\\"/g' \
                      | sed ':a;N;$!ba;s/\n/\\n/g' \
                      | cut -c1-"$MAX_SNIPPET_CHARS")

            entry=$(jq -n --argjson i "$issue" --arg snip "$snippet" '$i + {snippet: $snip}')
            if $FIRST; then FIRST=false; echo "$entry" >> issues.json; else echo ",$entry" >> issues.json; fi
          done < <(jq -c '.[]' trimmed-issues.json)
          echo ']' >> issues.json

          HAS_ISSUES=$(jq 'length > 0' issues.json)
          echo "has_issues=$HAS_ISSUES" >> $GITHUB_OUTPUT



      - name: Send issues to Technical Debt Analyzer (size-aware)
        id: flowise-analysis
        env:
          FLOWISE_BEARER_TOKEN: ${{ secrets.FLOWISE_BEARER_TOKEN }}
        run: |
          set -euo pipefail

          # ---- Tunables ----
          MAX_SUMMARY_CHARS=20000
          MAX_QUESTION_CHARS=32000
          MAX_PAYLOAD_BYTES=180000       # final JSON size safety cap (~180 KB)
          MAX_EXEMPLARS_PER_RULE=3       # if we switch to summary mode
          # -------------------

          ISSUES_JSON=$(cat issues.json)
          CHANGED_FILES=$(cat changed.json)

          if [[ "$ISSUES_JSON" == "[]" ]]; then
            echo '{"text": "No potential Technical Debt was detected in the current PR."}' > flowise_output.json
            exit 0
          fi

          # Compact Markdown summary
          SONAR_ISSUE_SUMMARY=$(jq -r '
            .[] |
            "### Rule: \(.rule)\n" +
            "* Severity: \(.severity)\n" +
            "* Location: \(.component):\(.line)\n" +
            "* Message: \(.message)\n" +
            (if .snippet then "* Snippet:*\\n```\\n\(.snippet)\\n```" else "" end) + "\n"
          ' issues.json | sed ':a;N;$!ba;s/\n/\\n/g' | head -c "$MAX_SUMMARY_CHARS")

          QUESTION_TEXT="You're analyzing SonarCloud issues from PR #${{ github.event.pull_request.number }} in '${{ github.repository }}' (owner: '${{ github.repository_owner }}'), targeting '${{ github.event.pull_request.head.ref }}'. If there are duplicates, analyze them once.\n\n## Sonar Issues (truncated)\n$SONAR_ISSUE_SUMMARY"
          QUESTION_TEXT=$(printf "%s" "$QUESTION_TEXT" | head -c "$MAX_QUESTION_CHARS")

          make_payload () {
            jq -cn \
              --arg pr "${{ github.event.pull_request.number }}" \
              --arg proj "${{ env.PROJECT_KEY }}" \
              --arg question "$QUESTION_TEXT" \
              --argjson issues "$1" \
              --argjson changedFiles "$2" \
              '{question:$question, overrideConfig:{projectKey:$proj, prNumber:$pr, changedFiles:$changedFiles}, issues:$issues}'
          }

          PAYLOAD=$(make_payload "$ISSUES_JSON" "$CHANGED_FILES")
          BYTES=$(printf '%s' "$PAYLOAD" | wc -c)
          echo "Initial payload bytes: $BYTES"

          # If still too big, progressively shrink:
          shrink_once () {
            local current="$1"
            # 1) Drop snippets first (keep structure)
            local no_snips
            no_snips=$(echo "$current" | jq 'map(del(.snippet))')
            echo "$no_snips"
          }

          if [ "$BYTES" -gt "$MAX_PAYLOAD_BYTES" ]; then
            echo "Dropping snippets to fit budget…"
            ISSUES_JSON=$(shrink_once "$ISSUES_JSON")
            PAYLOAD=$(make_payload "$ISSUES_JSON" "$CHANGED_FILES")
            BYTES=$(printf '%s' "$PAYLOAD" | wc -c)
            echo "After dropping snippets: $BYTES bytes"
          fi

          # If *still* too big, switch to per-rule summary with a few exemplars
          if [ "$BYTES" -gt "$MAX_PAYLOAD_BYTES" ]; then
            echo "Switching to per-rule summary mode…"
            SUMMARY_JSON=$(
              jq -c --argjson k "$MAX_EXEMPLARS_PER_RULE" '
                group_by(.rule)
                | map({
                    rule: .[0].rule,
                    severityMode: (.[0].severity),
                    count: length,
                    exemplars: (.[0:$k] | map({component, line, message}))
                  })
              ' <<<"$ISSUES_JSON"
            )

            QUESTION_TEXT=$(
              jq -r '
                map(
                  "### Rule: \(.rule) (" + (.count|tostring) + " hits, e.g.: " +
                  ( .exemplars | map("\(.component):\(.line)") | join(", ") ) + ")\n"
                ) | join("")
              ' <<<"$SUMMARY_JSON"
            )
            QUESTION_TEXT=$(printf "Summarized by rule due to size.\n\n%s" "$QUESTION_TEXT" | sed ':a;N;$!ba;s/\n/\\n/g' | head -c "$MAX_QUESTION_CHARS")

            # Replace issues with just the summary structure to *guarantee* small size
            PAYLOAD=$(jq -cn \
              --arg pr "${{ github.event.pull_request.number }}" \
              --arg proj "${{ env.PROJECT_KEY }}" \
              --arg question "$QUESTION_TEXT" \
              --argjson summary "$SUMMARY_JSON" \
              --argjson changedFiles "$CHANGED_FILES" \
              '{question:$question, overrideConfig:{projectKey:$proj, prNumber:$pr, changedFiles:$changedFiles}, issuesSummary:$summary}')
            BYTES=$(printf '%s' "$PAYLOAD" | wc -c)
            echo "After summary mode: $BYTES bytes"
          fi

          # Final send
          echo "======= DEBUG: FINAL PAYLOAD (sizes) ======="
          echo "question bytes: $(printf '%s' "$QUESTION_TEXT" | wc -c)"
          echo "payload bytes:  $BYTES"

          RESPONSE=$(curl -s -X POST \
            -H "Authorization: Bearer $FLOWISE_BEARER_TOKEN" \
            -H "Content-Type: application/json" \
            -d "$PAYLOAD" \
            https://cloud.flowiseai.com/api/v1/prediction/ae8b5cb1-b90b-4f25-9395-a839e92e2bf6)

          echo "$RESPONSE" > flowise_output.json



      - name: Comment on PR
        uses: actions/github-script@v6
        if: success()
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = 'flowise_output.json';

            if (!fs.existsSync(path)) {
              core.warning('Flowise output file not found.');
              return;
            }

            const raw = fs.readFileSync(path, 'utf8').trim();
            if (!raw || raw[0] !== '{') {
              core.warning('Flowise did not return valid JSON. Skipping comment.');
              return;
            }

            let parsed;
            try {
              parsed = JSON.parse(raw);
            } catch (e) {
              core.warning(`JSON parse error: ${e.message}`);
              return;
            }

            const body = parsed.text || 'Flowise returned:\n```json\n' +
                          JSON.stringify(parsed, null, 2) + '\n```';

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number, 
              body
            });
